
import pandas as pd
import numpy as numpy
import matplotlib.pyplot as pyplot
import seaborn as sns
dataset = pd.read_csv('titanic.csv')
dataset.head()
# The Dist Plot
sns.distplot(dataset['fare'],kde=False,bins=10)
# The Joint Plot
sns.jointplot(x='age', y='fare', data=dataset, kind='hex')
#The Rug Plot
sns.rugplot(dataset['fare'])
pyplot.show()


/*
Certainly! Here's an explanation for each line of code:

1. `get_ipython().system('pip install nltk')`: This line uses the `pip` package installer to install the NLTK library.

2. `import nltk`: This line imports the NLTK library.

3. `nltk.download('punkt')`: This line downloads the Punkt tokenizer data from NLTK, which is used for tokenization.

4. `from nltk.tokenize import sent_tokenize`: This line imports the `sent_tokenize` function from NLTK's `tokenize` module, which is used for sentence tokenization.

5. `text="""Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome. The sky is pinkish-blue. You shouldn't eat cardboard"""`: This line defines a multi-line string `text` containing some sample text for tokenization.

6. `tokenized_text=sent_tokenize(text)`: This line uses the `sent_tokenize` function to tokenize the text into sentences and stores the result in the `tokenized_text` variable.

7. `print(tokenized_text)`: This line prints the tokenized sentences.

8. `from nltk.tokenize import word_tokenize`: This line imports the `word_tokenize` function from NLTK's `tokenize` module, which is used for word tokenization.

9. `tokenized_word=word_tokenize(text)`: This line uses the `word_tokenize` function to tokenize the text into words and stores the result in the `tokenized_word` variable.

10. `print(tokenized_word)`: This line prints the tokenized words.

11. `from nltk.probability import FreqDist`: This line imports the `FreqDist` class from NLTK's `probability` module, which is used for frequency distribution analysis.

12. `fdist = FreqDist(tokenized_word)`: This line creates a `FreqDist` object `fdist` from the tokenized words.

13. `print(fdist)`: This line prints the frequency distribution.

14. `fdist.most_common(2)`: This line prints the most common words and their frequencies.

15. `import matplotlib.pyplot as plt`: This line imports the `pyplot` module from the `matplotlib` library, which is used for plotting.

16. `fdist.plot(30,cumulative=False)`: This line plots the frequency distribution, showing the top 30 words.

17. `plt.show()`: This line displays the plot.

18. `tokens=nltk.word_tokenize(sent)`: This line tokenizes the sentence into words using the `word_tokenize` function.

19. `print(tokens)`: This line prints the tokenized words.

20. `nltk.download('averaged_perceptron_tagger')`: This line downloads the averaged_perceptron_tagger data from NLTK, which is used for part-of-speech tagging.

21. `nltk.pos_tag(tokens)`: This line performs part-of-speech tagging on the tokens using the `pos_tag` function.

22. `nltk.download('stopwords')`: This line downloads the stopwords data from NLTK, which are common words that can be excluded from text analysis.

23. `stop_words = set(stopwords.words("english"))`: This line creates a set of stopwords for the English language using the `stopwords.words` function.

24. `print(stop_words)`: This line prints the set of stopwords.

25. `from nltk.corpus import stopwords`: This line imports the `stopwords` corpus from NLTK.

26. `from nltk.tokenize import word_tokenize`: This line imports the `word_tokenize` function from NLTK's `tokenize` module.

27. `example_sent = "Hello

 Mr. Smith, how are you doing today? The weather is great, and city is awesome. The sky is pinkish-blue. You shouldn't eat cardboard"`: This line defines a string `example_sent` containing some sample text for stopwords removal.

28. `stop_words = set(stopwords.words('english'))`: This line creates a set of stopwords for the English language.

29. `word_tokens = word_tokenize(example_sent)`: This line tokenizes the example sentence into words.

30. `filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]`: This line uses a list comprehension to filter out the stopwords from the tokenized words, ignoring case.

31. `filtered_sentence = []`: This line initializes an empty list to store the filtered words.

32. `for w in word_tokens:`: This line starts a loop to iterate over each word in the tokenized words.

33. `if w not in stop_words:`: This line checks if the word is not a stopword.

34. `filtered_sentence.append(w)`: This line adds the word to the `filtered_sentence` list.

35. `print(word_tokens)`: This line prints the tokenized words.

36. `print(filtered_sentence)`: This line prints the filtered words after stopwords removal.

37. `from nltk.corpus import stopwords`: This line imports the `stopwords` corpus from NLTK.

38. `from nltk.tokenize import word_tokenize`: This line imports the `word_tokenize` function from NLTK's `tokenize` module.

39. `text="Hello Mr. Smith, how are you doing today?"`: This line defines a string `text` containing a sample sentence.

40. `stop_words = set(stopwords.words("english"))`: This line creates a set of stopwords for the English language.

41. `tokenized_text=word_tokenize(text)`: This line tokenizes the text into words.

42. `filtered_sent=[]`: This line initializes an empty list to store the filtered words.

43. `for w in tokenized_text:`: This line starts a loop to iterate over each word in the tokenized words.

44. `if w not in stop_words:`: This line checks if the word is not a stopword.

45. `filtered_sent.append(w)`: This line adds the word to the `filtered_sent` list.

46. `print("Tokenized Sentence:",tokenized_text)`: This line prints the tokenized sentence.

47. `print("Filterd Sentence  :",filtered_sent)`: This line prints the filtered sentence after stopwords removal.

48. `removed_words = []`: This line initializes an empty list to store the removed stopwords.

49. `for w in tokenized_text:`: This line starts a loop to iterate over each word in the tokenized words.

50. `if w in stop_words:`: This line checks if the word is a stopword.

51. `removed_words.append(w)`: This line adds the word to the `removed_words` list.

52. `print("Filterd Sentence  :",removed_words)`: This line prints the removed stopwords.

53. `from nltk.stem import PorterStemmer`: This line imports the `PorterStemmer` class from NLTK's `stem` module, which is used for stemming.

54. `from nltk.tokenize import sent_tokenize, word_tokenize`: This line imports the `sent_tokenize` and `word_tokenize` functions from NLTK's `tokenize` module.

55. `ps = PorterStemmer()`: This line creates an instance of the `PorterStemmer` class.

56. `stemmed_words=[]`: This line initializes an empty list to store the stemmed words.

57

. `for w in filtered_sent:`: This line starts a loop to iterate over each word in the filtered words.

58. `stemmed_words.append(ps.stem(w))`: This line applies stemming to each word using the `stem` method of the `PorterStemmer` class and adds the stemmed word to the `stemmed_words` list.

59. `print("Filtered Sentence:",filtered_sent)`: This line prints the filtered sentence.

60. `print("Stemmed Sentence:",stemmed_words)`: This line prints the stemmed sentence.

61. `import nltk`: This line imports the NLTK library.

62. `nltk.download('wordnet')`: This line downloads the WordNet data from NLTK, which is used for lemmatization.

63. `import nltk`: This line imports the NLTK library.

64. `nltk.download('omw-1.4')`: This line downloads the Open Multilingual WordNet (OMW) data from NLTK.

65. `from nltk.stem.wordnet import WordNetLemmatizer`: This line imports the `WordNetLemmatizer` class from NLTK's `wordnet` module, which is used for lemmatization.

66. `lem = WordNetLemmatizer()`: This line creates an instance of the `WordNetLemmatizer` class.

67. `from nltk.stem.porter import PorterStemmer`: This line imports the `PorterStemmer` class from NLTK's `stem` module, which is used for stemming.

68. `stem = PorterStemmer()`: This line creates an instance of the `PorterStemmer` class.

69. `word = "flying"`: This line defines a string `word` to be lemmatized and stemmed.

70. `print("Lemmatized Word:",lem.lemmatize(word,"v"))`: This line lemmatizes the word using the `lemmatize` method of the `WordNetLemmatizer` class and prints the lemmatized word.

71. `print("Stemmed Word:",stem.stem(word))`: This line stems the word using the `stem` method of the `PorterStemmer` class and prints the stemmed word.

72. `import pandas as pd`: This line imports the `pandas` library and assigns it the alias `pd`.

73. `import sklearn as sk`: This line imports the `sklearn` library and assigns it the alias `sk`.

74. `import math`: This line imports the `math` module.

75. `first_sentence = "Data Science is the sexiest job of the 21st century"`: This line defines a string `first_sentence` containing the first sentence for TF-IDF analysis.

76. `second_sentence = "machine learning is the key for data science"`: This line defines a string `second_sentence` containing the second sentence for TF-IDF analysis.

77. `first_sentence = first_sentence.split(" ")`: This line splits the `first_sentence` string into a list of words based on whitespace.

78. `second_sentence = second_sentence.split(" ")`: This line splits the `second_sentence` string into a list of words based on whitespace.

79. `total= set(first_sentence).union(set(second_sentence))`: This line creates a set `total` that contains all unique words from both sentences.

80. `print(total)`: This line prints the set of unique words.

81. `wordDictA = dict.fromkeys(total, 0)`: This line creates a dictionary `wordDictA` with keys as the words from `total` set and initial values set

 to 0.

82. `wordDictB = dict.fromkeys(total, 0)`: This line creates a dictionary `wordDictB` with keys as the words from `total` set and initial values set to 0.

83. `for word in first_sentence:`: This line starts a loop to iterate over each word in the `first_sentence` list.

84. `wordDictA[word]+=1`: This line increments the count of the word in `wordDictA` dictionary.

85. `for word in second_sentence:`: This line starts a loop to iterate over each word in the `second_sentence` list.

86. `wordDictB[word]+=1`: This line increments the count of the word in `wordDictB` dictionary.

87. `def computeTF(wordDict, bow):`: This line defines a function `computeTF` that calculates the term frequency (TF) of words in a bag-of-words (bow).

88. `def computeIDF(docList):`: This line defines a function `computeIDF` that calculates the inverse document frequency (IDF) of words in a list of documents (docList).

89. `def computeTFIDF(tfBow, idfs):`: This line defines a function `computeTFIDF` that calculates the TF-IDF scores given the term frequency (TF) and inverse document frequency (IDF).

90. `tfBowA = computeTF(wordDictA, first_sentence)`: This line calculates the TF for `first_sentence` using the `computeTF` function and assigns it to `tfBowA`.

91. `tfBowB = computeTF(wordDictB, second_sentence)`: This line calculates the TF for `second_sentence` using the `computeTF` function and assigns it to `tfBowB`.

92. `idfs = computeIDF([wordDictA, wordDictB])`: This line calculates the IDF using the `computeIDF` function and assigns it to `idfs`.

93. `tfidfBowA = computeTFIDF(tfBowA, idfs)`: This line calculates the TF-IDF scores for `first_sentence` using the `computeTFIDF` function and assigns it to `tfidfBowA`.

94. `tfidfBowB = computeTFIDF(tfBowB, idfs)`: This line calculates the TF-IDF scores for `second_sentence` using the `computeTFIDF` function and assigns it to `tfidfBowB`.

95. `df = pd.DataFrame([tfidfBowA, tfidfBowB])`: This line creates a pandas DataFrame `df` from the TF-IDF scores.

96. `print(df)`: This line prints the DataFrame.

These lines of code demonstrate various functionalities of the NLTK library, such as tokenization, stemming, lemmatization, stopwords removal, part-of-speech tagging, and TF-IDF analysis.

*/

